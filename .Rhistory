<<<<<<< HEAD
=======
ggplot(data = filter(gme_date_discred))+  #, Date > "2022-02-05")
geom_point(mapping = aes(x = Date, y = mean, col = sum), show.legend = TRUE)+
scale_color_gradient(low="red", high="green") +
geom_hline(yintercept = 0)
sum(gme_date_discred$sum)
import tidyverse
package(tidyverse)
library(tidyverse)
df = read_csv("pushshift.csv")
head(df)
view(df)
as.Date(df$cr)
as.Date(df$created)
test = 0as.Date(df$created)
test = as.Date(df$created)
library(readr)
pushshift <- read_csv("pushshift.csv", col_types = cols(created = col_character()))
View(pushshift)
as.Date(pushshift$created)
as.Date(pushshift$created, "%Y%m%d")
as.Date(as.character(pushshift$created), "%Y%m%d")
as.Date(as.character(pushshift$created))
strptime(pushshift$created))
strptime(pushshift$created)
strptime(pushshift$created, format = "%Y%m%d")
ymd(pushshift$created)
as.Date(as.POSIXct(pushshift$created, origin="1970-01-01"))
as.Date(as.POSIXct(pushshift$created_utc, origin="1970-01-01"))
meine_date = read_csv("pushshift.csv")
meine_date$created_utc = as.Date(as.POSIXct(pushshift$created_utc, origin="1970-01-01"))
meine_date$created = as.Date(as.POSIXct(pushshift$created, origin="1970-01-01"))
View(df)
meine_date = meine_date[-"created"]
meine_date = meine_date[-c("created")]
meine_date = meine_date[,-c("created")]
meine_date = meine_date[,-created]
meine_date = meine_date[-created]
meine_date = select(meine_date, -created)
meine_date = select(meine_date, -c(created,-d_))
meine_date = select(meine_date, -d_))
meine_date = select(meine_date, -d_
meine_date = select(meine_date, -d_)
meine_date = select(meine_date, -d_)
head(meine_date)
n = nrow(meine_date)
meine_date[n]
meine_date[n-1]
meine_date[n,]
meine_date[4000,]
ggplot(data = meine_date, mapping = aes(x=created_utc, y = num_comments))+
geom_line()
ggplot(data = meine_date, mapping = aes(x=created_utc, y = num_comments))+
geom_point()
data = read_csv("pushshift.csv")
data$created_utc = as.Date(as.POSIXct(data$created_utc, origin="1970-01-01"))
data = select(meine_date, c(-d_, - created))
data = select(meine_date, -c(d_,created))
data = select(data, -c(d_,created))
n = nrow(data)
ggplot(data = data, mapping = aes(x=created_utc, y = num_comments))+
geom_point()
dim(data)
sum(data$num_comments)
write.csv(data, "pushshift_clean.csv")
##### Find Threads von python ----------
threads = read.csv("pushshift_clean.csv")
#write.csv(x = thread_url, file = "thread_day.csv", sep = ";")
head(threads)
n = nrow(threads)
##### find threads--------
gme_tibble_mean = tibble(Date = threads$created_utc,
Count_of_Comments = threads$num_comments,
URL =  threads$url,
mean = rep(0))
##### Librarys ---------
library(syuzhet)
library(tidyverse)
##### find threads--------
gme_tibble_mean = tibble(Date = threads$created_utc,
Count_of_Comments = threads$num_comments,
URL =  threads$url,
mean = rep(0))
gme_tibble_mean$URL[1]
get_thread_content(gme_tibble_mean$URL[1])
##### Librarys ---------
library(syuzhet)
library(RedditExtractoR)
get_thread_content(gme_tibble_mean$URL[1])
test = get_thread_content(gme_tibble_mean$URL[1])
nrow(test$comments)
test = get_thread_content(gme_tibble_mean$URL[10])
nrow(test$comments)
test$comments$comment[1]
test$comments$comment[2]
test$comments$comment[3]
test$comments$comment[4]
test$comments$comment[5]
get_sentences(test$comments$comment[5])
##### analyse thread -----------
summarize_thread = function(content_array){
anzahl_comments = nrow(content_array$comments)
>>>>>>> be6a703d9ea0f75eebd914c2d90e5f710b76f9b9
sentiment = rep(0,anzahl_comments-1)
for(j in 2:anzahl_comments){
comment = content_array$comments$comment[j]
saetze = get_sentences(comment)
sentiment[j-1] = mean(get_sentiment(saetze))*content_array$comments$score[j] #weighted by score
}
return(mean(sentiment))
}
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)
}
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
try(thread_content = get_thread_content(gme_tibble_mean$URL[i]))
gme_tibble_mean$mean[i] = summarize_thread(thread_content)
}
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)}, finally ={})
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)}, finally ={})
}
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){print("Error")},
finally ={})
}
printf("%d", 5)
flush.console()
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
summ_err = 0
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
flush.console()
print(summ_err)
summ_err = summ_err + 1},
finally ={})
}
print('\r')
print('\r)
print('\r)
print(\r)
print(\r)
\r
print("\r ")
print(tets)
print("tets")
print("\r ")
print(\r summ_err)
print("\r"+ summ_err)
print("\r"_err)
print(summ_err + i)
print(summ_err + "von" + i)
print(summ_err/i)
i
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
summ_err = 0
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
flush.console()
summ_err = summ_err + 1
print(summ_err + i)
},
finally ={})
}
sprintf("%d" + "Hallo", 6)
sprintf(%d + "Hallo", 6)
sprintf(%s + "Hallo", 6)
sprintf("Hallo + %s", 10)
summ_err = 0
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
flush.console()
summ_err = summ_err + 1
sprintf("\r%s Erro im Lauf %s", summ_err, i)
},
finally ={})
}
summ_err = 0
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
flush.console()
summ_err = summ_err + 1
sprintf("%s Erro im Lauf %s\r", summ_err, i)
},
finally ={})
}
sprintf("%s Erro im Lauf %s", summ_err, i)
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
flush.console()
summ_err = summ_err + 1
sprintf("%s Erro im Lauf %s", summ_err, i)
},
finally ={})
}
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
flush.console()
summ_err = summ_err + 1
print("hallo")
sprintf("%s Erro im Lauf %s", summ_err, i)
},
finally ={})
}
sprintf("%s Erro im Lauf %s", summ_err, i)
summ_err
string = 10 + "Test"
print(sprintf("%s Erro im Lauf %s", summ_err, i))
summ_err = 1
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
for (i in 1:n) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
summ_err = summ_err + 1
print(sprintf("%s Erro im Lauf %s", summ_err, i))
},
finally ={})
}
View(gme_tibble_mean)
for (i in 1:10) { # gehe durch alle threads
setTxtProgressBar(pb, i)
summ_err = 10 * tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
print(sprintf("%s Erro im Lauf %s", summ_err, i))
return(1)
},
finally ={})
}
summ_err = 1
for (i in 1:10) { # gehe durch alle threads
setTxtProgressBar(pb, i)
summ_err = 10 * tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
print(sprintf("%s Erro im Lauf %s", summ_err, i))
return(1)
},
finally ={})
}
sum(ends_with(data$url)
data %>% ends_with(".jpg")
data %>% ends_with(".jpg")
data$url %>% ends_with(".jpg")
select(data %>% ends_with(".jpg"))
select(data %>% ends_with(".jpg"))
data = data[-(endsWith(data$url, "jpg"))]
endsWith(data$url, "jpg")
data = data[!(endsWith(data$url, "jpg"))]
data = data[!(endsWith(data$url, "jpg"))]
data = data[!(endsWith(data$url, "jpg")),]
view(data)
data = data[!(endsWith(data$url, "png")),]
view(data)
write.csv(data, "pushshift_clean.csv")
write.csv(data, "pushshift_clean.csv")
##### Find Threads von python ----------
threads = read.csv("pushshift_clean.csv")
n = nrow(threads)
##### find threads--------
gme_tibble_mean = tibble(Date = threads$created_utc,
Count_of_Comments = threads$num_comments,
URL =  threads$url,
mean = rep(0))
anzahl_comments = nrow(content_array$comments)
##### analyse thread -----------
summarize_thread = function(content_array){
anzahl_comments = nrow(content_array$comments)
sentiment = rep(0,anzahl_comments-1)
for(j in 2:anzahl_comments){
comment = content_array$comments$comment[j]
saetze = get_sentences(comment)
sentiment[j-1] = mean(get_sentiment(saetze))*content_array$comments$score[j] #weighted by score
}
return(mean(sentiment))
}
pb = txtProgressBar(min = 0, max = 150, initial = 0, style = 3)
for (i in 1:150) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
print(sprintf("Erro im Lauf %s", i))
},
finally ={})
}
ggplot(data = gme_tibble_mean[1:150,])+
geom_point(mapping = aes(x = Date, y = mean, col = Count_of_Comments), show.legend = TRUE)+
scale_color_gradient(low="red", high="green")
fruit <- c("apple", "banana", "pear", "pinapple")
str_detect(fruit, "a")
data = data[str_detect(data$url, "comments")]
data = data[str_detect(data$url, "comments"),]
view(data)
data$url[100]
data %>% select(Count_of_Comments > 2)
names(data)
data %>% select(num_comments > 2)
data %>% select(data$num_comments > 2)
data %>% filter(data$num_comments > 2)
data = data %>% filter(data$num_comments > 2)
write.csv(data, "pushshift_clean.csv")
##### Find Threads von python ----------
threads = read.csv("pushshift_clean.csv")
n = nrow(threads)
##### find threads--------
gme_tibble_mean = tibble(Date = threads$created_utc,
Count_of_Comments = threads$num_comments,
URL =  threads$url,
mean = rep(0))
##### analyse thread -----------
summarize_thread = function(content_array){
anzahl_comments = nrow(content_array$comments)
sentiment = rep(0,anzahl_comments-1)
for(j in 2:anzahl_comments){
comment = content_array$comments$comment[j]
saetze = get_sentences(comment)
sentiment[j-1] = mean(get_sentiment(saetze))*content_array$comments$score[j] #weighted by score
}
return(mean(sentiment))
}
pb = txtProgressBar(min = 0, max = 150, initial = 0, style = 3)
for (i in 1:150) { # gehe durch alle threads
setTxtProgressBar(pb, i)
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
print(sprintf("Erro im Lauf %s", i))
},
finally ={})
}
ggplot(data = gme_tibble_mean[1:150,])+
geom_point(mapping = aes(x = Date, y = mean, col = Count_of_Comments), show.legend = TRUE)+
scale_color_gradient(low="red", high="green")
data = read_csv("pushshift.csv")
data$created_utc = as.Date(as.POSIXct(data$created_utc, origin="1970-01-01"))
View(data)
summary(data$created_utc)
unique(data$created_utc)
library(tidyverse)
data = read_csv("pushshift.csv")
data$created_utc = as.Date(as.POSIXct(data$created_utc, origin="1970-01-01"))
data = select(data, -c(d_,created))
data = data[str_detect(data$url, "comments"),] #url ohne comment sind bilder/memes/videos
data = data %>% filter(data$num_comments > 2) ##weniger als 3 Kommentare entweder irrelvant, gelöscht oder Bot
dim(data)
Anzahl_Kommentare = sum(data$num_comments)
head(data)
boxplot(data$num_comments)
which.max(data$num_comments)
boxplot(data$num_comments)
data[ 7425, ]
data[ 7425, ]$url
hist(data$num_comments)
hist(data$num_comments, breaks = 10)
hist(data$num_comments, breaks = 100)
hist(data$num_comments, breaks = 1000)
hist(data$num_comments, breaks = 1000, xlim = 5000)
hist(data$num_comments, breaks = 1000, ylim = 5000)
hist(data$num_comments, breaks = 1000,xlim = 1:5000)
hist(data$num_comments, breaks = 1000,xlim = range(1,5000)
hist(data$num_comments, breaks = 1000,xlim = range(1,5000))
hist(data$num_comments, breaks = 1000,xlim = range(1,5000))
count(data %>% select(data$num_comments > 100))
count(data %>% filter(data$num_comments > 100))
count(data %>% filter(data$num_comments > 1000))
count(data %>% filter(data$num_comments > 10000))
count(data %>% filter(data$num_comments > 100000))
summary(data$num_comments)
quantile(data$num_comments)
quantile(data$num_comments, probs = 0.01)
quantile(data$num_comments, type = 9)
quantile(data$num_comments, probs = seq(0,1,by = 0.01))
quantile(data$num_comments, probs = seq(0,1,by = 0.1))
count(data %>% filter(data$num_comments > 100000))
count(data %>% filter(data$num_comments > 1000))
hist(data$num_comments,xlim = range(0,1000), breaks = 25)
data_under_1000 = data %>% filter(data$num_comments > 1000)
hist(data_under_1000 , breaks = 25)
hist(data_under_1000$num_comments , breaks = 25)
data_under_1000 = data %>% filter(data$num_comments > 1000)
hist(data_under_1000$num_comments , breaks = 25)
data_under_1000 = data %>% filter(data$num_comments < 1000)
hist(data_under_1000$num_comments , breaks = 25)
sum(data_under_1000$num_comments)
Anzahl_Kommentare
library(tidyverse)
names(data)
ggplot(data = data, mapping = aes(x = created_utc, y = num_comments))
ggplot(data = data, mapping = aes(x = created_utc, y = num_comments)) + geom_point(col= data$score)
ggplot(data = data, mapping = aes(x = created_utc, y = num_comments), col = score) + geom_point()
ggplot(data = data, mapping = aes(x = created_utc, y = num_comments), col = score) + geom_point(show.legend =  T)
ggplot(data = data_under_1000, mapping = aes(x = created_utc, y = num_comments), col = score) + geom_point(show.legend =  T)
ggplot(data = data_under_1000, mapping = aes(x = created_utc, y = num_comments), col = score) + geom_point(size = 0.1)
ggplot(data = data_under_1000, mapping = aes(x = created_utc, y = num_comments, col = score)) + geom_point(size = 0.1)
ggplot(data = data_under_1000, mapping = aes(x = created_utc, y = num_comments, col = score)) +
geom_point(size = 0.1) +
scale_color_gradient(low = "red", high ="green")
ggplot(data = data_under_1000, mapping = aes(x = created_utc, y = num_comments, col = score)) +
geom_point(size = 1) +
scale_color_gradient(low = "red", high ="green")
ggplot(data = data_under_1000, mapping = aes(x = created_utc, y = num_comments, col = score)) +
geom_point(size = 1) +
scale_color_gradient(low = "red", high ="blue")
data_over_1000 =  data %>% filter(data$num_comments > 1000)
hist(data_over_1000$num_comments , breaks = 25)
sum(data_over_1000$num_comments)
ggplot(data = data_under_1000, mapping = aes(x = created_utc, y = num_comments, col = score)) +
geom_point(size = 1) +
scale_color_gradient(low = "red", high ="blue")
ggplot(data = data_over_1000, mapping = aes(x = created_utc, y = num_comments, col = score)) +
geom_point(size = 1) +
scale_color_gradient(low = "red", high ="blue")
library(tidyverse)
data = read_csv("pushshift.csv")
data$created_utc = as.Date(as.POSIXct(data$created_utc, origin="1970-01-01"))
data = select(data, -c(d_,created))
ggplot(data = data, mapping = aes(x=created_utc, y = num_comments))+
geom_point()
data = data[str_detect(data$url, "comments"),] #url ohne comment sind bilder/memes/videos
data = data %>% filter(data$num_comments > 2) ##weniger als 3 Kommentare entweder irrelvant, gelöscht oder Bot
dim(data)
Anzahl_Kommentare = sum(data$num_comments)
write.csv(data, "pushshift_clean.csv")
### cleaning hier fertig, aber anzahl der kommentare müssen reduziert werden
head(data)
data_under_1000 = data %>% filter(data$num_comments < 1000)
data_over_1000 =  data %>% filter(data$num_comments > 1000)
hist(data_under_1000$num_comments , breaks = 10)
sum(data_under_1000$num_comments)
ggplot(data = data_under_1000, mapping = aes(x = created_utc, y = num_comments, col = score)) +
geom_point(size = 1) +
scale_color_gradient(low = "red", high ="blue")
<<<<<<< HEAD
data = read_csv("pushshift.csv")
library(tidyverse)
sentiment_frame = read_csv("gme_date_discrete.csv")
view(sentiment_frame)
sentiment_frame = sentiment_frame[,-1]
sentiment_frame
sentiment_frame = sentiment_frame %>% mutate(week_day = weekdays((Date)))
sentiment_frame
sentiment_frame = sentiment_frame
week_day_add =sentiment_frame %>% if (Date="Sonntag"){
return(2)
} else if(Date = "Samstag"){
return(3)
} else if(Date = "Freitag"){
return(4)
} else if(Date = "Donnerstag"){
return(4)
} else {
return(2)
}
week_day_add =sentiment_frame %>% if (Date="Sonntag"){
return(2)
} else if(Date = "Samstag"){
return(3)
} else if(Date = "Freitag"){
return(4)
} else if(Date = "Donnerstag"){
return(4)
} else {
return(2)
}
week_day_add = if (sentiment_frame$Date="Sonntag"){
return(2)
} else if(. = "Samstag"){
return(3)
} else if(Date = "Freitag"){
return(4)
} else if(Date = "Donnerstag"){
return(4)
} else {
return(2)
}
week_day_add = if (sentiment_frame$Date="Sonntag"){
return(2)
} else if(. = "Samstag"){
return(3)
} else if(Date = "Freitag"){
return(4)
} else if(Date = "Donnerstag"){
return(4)
} else {
return(2)
}
week_day_add = if (sentiment_frame$Date="Sonntag"){
2
} else if(. = "Samstag"){
3
} else if(Date = "Freitag"){
4
} else if(Date = "Donnerstag"){
4
} else {
2
}
week_day_add = if (sentiment_frame$Date="Sonntag"){
2
} else if(sentiment_frame$Date = "Samstag"){
3
} else if(sentiment_frame$Date = "Freitag"){
4
} else if(sentiment_frame$Date = "Donnerstag"){
4
} else {
2
}
week_day_add = rep(2,nrow(sentiment_frame))
week_day_add = if(sentiment_frame$Date = "Samstag") 3
3
week_day_add = if(sentiment_frame$Date = "Samstag"){
3
}
week_day_add = 3
week_day_add = rep(2,nrow(sentiment_frame))
week_day_add = if(sentiment_frame$Date = "Samstag"){
week_day_add = 3
}
week_day_add = rep(2,nrow(sentiment_frame))
week_day_add[sentiment_frame$Date = "Samstag"] = 3
week_day_add[sentiment_frame$Date == "Samstag"] = 3
sentiment_frame$Date == "Samstag"
sentiment_frame$week_day == "Samstag"
week_day_add[sentiment_frame$week_day == "Samstag"] = 3
sentiment_frame = read_csv("gme_date_discrete.csv")
sentiment_frame = sentiment_frame[,-1]
sentiment_frame = sentiment_frame %>% mutate(week_day = weekdays((Date)))
week_day_add = rep(2,nrow(sentiment_frame))
week_day_add[sentiment_frame$week_day == "Samstag"] = 3
week_day_add[sentiment_frame$week_day == "Freitag"] = 4
week_day_add[sentiment_frame$week_day == "Donnerstag"] = 4
sentiment_frame = sentiment_frame %>% mutate(week_day_add = week_day_add)
view(sentiment_frame)
sentiment_frame = sentiment_frame %>% mutate(day_to_pred = Date + week_day_add)
view(sentiment_frame)
write.csv(sentiment_frame, "sentiment_frame.csv")
#----Librarys
library(tidyverse)
gme_sap = read.csv(file = "controlls_and_gme.R")
gme_sap = read_csv(file = "controlls_and_gme.R")
#----Librarys
library(tidyverse)
gme_sap = read_csv(file = "controlls_and_gme.R")
gme_sap
gme_sap = read_csv(file = "Stock_Data.csv")
sentiment = read_csv(file = "sentiment_frame.csv")
sentiment = sentiment %>% select(-Data)
sentiment = sentiment %>% select(-Date)
sentiment
names(sentiment)
sentiment = sentiment %>% select(-c(Date,...1))
sentiment = read_csv(file = "sentiment_frame.csv")
sentiment = sentiment %>% select(-c(Date,...1))
sentiment = sentiment[,-1]
sentiment
full_data = inner_join(gme_sap, sentiment, by = c ("Date" = "day_to_pred"))
full_data
full_data = full_data %>% mutate(-c(Close.x, lag, Close.y, lag1, lag2,lag3))
full_data = full_data %>% select(-c(Close.x, lag, Close.y, lag1, lag2,lag3))
full_data
gme_sap = read_csv(file = "Stock_Data.csv")
sentiment = read_csv(file = "sentiment_frame.csv")
sentiment = sentiment %>% select(-Date)
sentiment = sentiment[,-1]
full_data = inner_join(gme_sap, sentiment, by = c ("Date" = "day_to_pred"))
full_data
full_data = full_data %>% select(-c(Close.x, lag, Close.y, lag1, lag2,lag3))
full_data
lm_reg = lm(formula = gme_change~mean +sum_com + mean*sum_com + sp_change  +lag(sp_change,1) + lag(sp_change, 2) + lag(sp_change, 3),
data = full_data)
summary(lm_reg)
plot(x = mean, y = gme_change, data = full_data)
plot(x = mean, y = gme_change, data = full_data)
ggplot(data = full_data) + geom_point(mapping = aes(x=mean, y = gme_change))
#---- Regression
lm_reg = lm(formula = gme_change~mean +sum_com + mean*sum_com, data = full_data)
summary(lm_reg)
summary(lm_reg)
ggplot(data = full_data) + geom_point(mapping = aes(x=mean, y = gme_change))
=======
##### Librarys ---------
library(syuzhet)
library(tidyverse)
library(RedditExtractoR)
##### Find Threads von python ----------
threads = read.csv("pushshift_clean.csv")
n = nrow(threads)
##### group Threads-------
gme_tibble_mean = tibble(Date = threads$created_utc,
Count_of_Comments = threads$num_comments,
URL =  threads$url,
mean = rep(0))
##### analyse thread -----------
summarize_thread = function(content_array){
anzahl_comments = nrow(content_array$comments)
sentiment = rep(0,anzahl_comments-1)
for(j in 2:anzahl_comments){
comment = content_array$comments$comment[j]
saetze = get_sentences(comment)
sentiment[j-1] = mean(get_sentiment(saetze))*content_array$comments$score[j] #weighted by score
}
return(mean(sentiment))
}
pb = txtProgressBar(min = 0, max = n, initial = 0, style = 3)
for (i in 1:n) { # gehe durch alle threads
tryCatch(expr = {thread_content = get_thread_content(gme_tibble_mean$URL[i])
gme_tibble_mean$mean[i] = summarize_thread(thread_content)},
error = function(e){
print(sprintf("Error im Lauf %s", i))
},
finally ={setTxtProgressBar(pb, i)})
}
close(pb)
ggplot(data = gme_tibble_mean[1:n,])+
geom_point(mapping = aes(x = Date, y = mean, col = Count_of_Comments), show.legend = TRUE)+
scale_color_gradient(low="red", high="green")
##### Sorge dafür, dass es nur einen wert pro tag gibt ----
gme_tibble_mean = na.omit(gme_tibble_mean)
gme_date_discrete = select(gme_tibble_mean, - "URL") %>% group_by(Date) %>% summarise(mean = weighted.mean(x = mean, w = Count_of_Comments ), sum_com = sum(Count_of_Comments))
#save data
write.csv(gme_date_discrete, file = "gme_date_discrete.csv")
#plot
ggplot(data = filter(gme_date_discrete))+  #, Date > "2022-02-05")
geom_point(mapping = aes(x = Date, y = mean, col = sum_com), show.legend = TRUE)+
scale_color_gradient(low="red", high="green") +
geom_hline(yintercept = 0)
>>>>>>> be6a703d9ea0f75eebd914c2d90e5f710b76f9b9
